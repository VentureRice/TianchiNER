{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF+LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras 2.2.4\n",
    "\n",
    "tensorflow 1.13\n",
    "\n",
    "pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab_path = \"CRF/data/char_vocabs.txt\" # 字典文件\n",
    "train_data_path = 'CRF/train_data/train_data_000' # 训练数据\n",
    "#train_data_path = './data/train_data' # 训练数据\n",
    "test_data_path = 'CRF/train_data/train_data_000' # 测试数据\n",
    "\n",
    "special_words = ['<PAD>', '<UNK>'] # 特殊词表示\n",
    "\n",
    "# \"BIO\"标记的标签\n",
    "#label2idx = {\"O\": 0,\n",
    "#             \"B-PER\": 1, \"I-PER\": 2,\n",
    "#             \"B-LOC\": 3, \"I-LOC\": 4,\n",
    "#             \"B-ORG\": 5, \"I-ORG\": 6\n",
    "#            }\n",
    "label2idx = {'O': 0,\n",
    "             'DISEASE': 1, 'DISEASE_GROUP': 2,\n",
    "             'DRUG_DOSAGE': 3, 'DRUG_EFFICACY': 4,\n",
    "             'DRUG_INGREDIENT': 5, 'DRUG_TASTE': 6,\n",
    "             'FOOD_GROUP':7, 'PERSON_GROUP':8,\n",
    "             'SYMPTOM':9, 'SYNDROME':10\n",
    "            }\n",
    "\n",
    "# 索引和BIO标签对应\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "\n",
    "# 读取字符词典文件\n",
    "with open(char_vocab_path, \"r\", encoding=\"utf8\") as fo:\n",
    "    char_vocabs = [line.strip() for line in fo]\n",
    "char_vocabs = special_words + char_vocabs\n",
    "\n",
    "# 字符和索引编号对应\n",
    "idx2vocab = {idx: char for idx, char in enumerate(char_vocabs)}\n",
    "vocab2idx = {char: idx for idx, char in idx2vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CRF/train_data/train_data_000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3379a0cbeedd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 加载训练集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_datas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# 加载测试集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtest_datas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3379a0cbeedd>\u001b[0m in \u001b[0;36mread_corpus\u001b[0;34m(corpus_path, vocab2idx, label2idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 读取训练语料\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CRF/train_data/train_data_000'"
     ]
    }
   ],
   "source": [
    "# 读取训练语料\n",
    "def read_corpus(corpus_path, vocab2idx, label2idx):\n",
    "    with open(corpus_path, encoding='utf-8') as fr:\n",
    "        lines = fr.readlines()\n",
    "\n",
    "    sent_, tag_ = [], []\n",
    "    for letter in lines:\n",
    "        [char,label,_] = re.split('\\t|\\n',letter)\n",
    "        sent_.append(char)\n",
    "        tag_.append(label)\n",
    "\n",
    "    sent_ids = [vocab2idx[char] if char in vocab2idx else vocab2idx['<UNK>'] for char in sent_]\n",
    "    tag_ids = [label2idx[label] if label in label2idx else 0 for label in tag_]\n",
    "    return sent_ids, tag_ids\n",
    "\n",
    "# 加载训练集\n",
    "train_datas, train_labels = read_corpus(train_data_path, vocab2idx, label2idx)\n",
    "# 加载测试集\n",
    "test_datas, test_labels = read_corpus(test_data_path, vocab2idx, label2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = []\n",
    "train_labels = []\n",
    "for i in range(1000):\n",
    "    train_data_path_i = './train_data/train_data_%03d'%i\n",
    "    train_datas_i, train_labels_i = read_corpus(train_data_path_i, vocab2idx, label2idx)\n",
    "    train_datas.append(train_datas_i)\n",
    "    train_labels.append(train_labels_i)\n",
    "    #if i%10==0:\n",
    "    #    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datas = train_datas\n",
    "test_labels = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_datas[50])\n",
    "print([idx2vocab[idx] for idx in train_datas[50]])\n",
    "print(train_labels[50])\n",
    "print([idx2label[idx] for idx in train_labels[50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Masking, Embedding, Bidirectional, LSTM, Dense, Input, TimeDistributed, Activation\n",
    "from keras.preprocessing import sequence\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 32\n",
    "HIDDEN_SIZE = 16\n",
    "MAX_LEN = 500\n",
    "VOCAB_SIZE = len(vocab2idx)\n",
    "CLASS_NUMS = len(label2idx)\n",
    "print(VOCAB_SIZE, CLASS_NUMS)\n",
    "\n",
    "print('padding sequences')\n",
    "train_datas = sequence.pad_sequences(train_datas, maxlen=MAX_LEN)\n",
    "train_labels = sequence.pad_sequences(train_labels, maxlen=MAX_LEN)\n",
    "test_datas = sequence.pad_sequences(test_datas, maxlen=MAX_LEN)\n",
    "test_labels = sequence.pad_sequences(test_labels, maxlen=MAX_LEN)\n",
    "print('x_train shape:', train_datas.shape)\n",
    "print('x_test shape:', test_datas.shape)\n",
    "\n",
    "train_labels = keras.utils.to_categorical(train_labels, CLASS_NUMS)\n",
    "test_labels = keras.utils.to_categorical(test_labels, CLASS_NUMS)\n",
    "print('trainlabels shape:', train_labels.shape)\n",
    "print('testlabels shape:', test_labels.shape)\n",
    "\n",
    "## BiLSTM+CRF模型构建\n",
    "inputs = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "x = Masking(mask_value=0)(inputs)\n",
    "x = Embedding(VOCAB_SIZE, EMBED_DIM, mask_zero=True)(x)\n",
    "x = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True))(x)\n",
    "x = TimeDistributed(Dense(CLASS_NUMS))(x)\n",
    "outputs = CRF(CLASS_NUMS)(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=crf_loss, optimizer='adam', metrics=[crf_viterbi_accuracy])\n",
    "model.fit(train_datas, train_labels, epochs=EPOCHS, verbose=1, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(test_datas, test_labels, batch_size=BATCH_SIZE)\n",
    "print(model.metrics_names)\n",
    "print(score)\n",
    "\n",
    "# save model\n",
    "model.save(\"./model/ch_ner_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 500\n",
    "sentence = \" 北京同仁堂科技发展股份有限公司制药厂  1.忌食辛辣，少进油腻。 2.感冒发热病人不宜服用。 3.有高血压、心脏病、肝病、糖尿病、肾病等慢性病严重者应在医师指导下服用。 4.伴有月经紊乱者，应在医师指导下服用。 5.眩晕症状较重者，应及时去医院就诊。 6.服药2周症状无缓解，应去医院就诊。 7.对本品过敏者禁用，过敏体质者慎用。 8.本品性状发生改变时禁止使用。 9.请将本品放在儿童不能接触的地方。 10.如正在使用其他药品，使用本品前请咨询医师或药师。  本品为浅黄色至棕黄色颗粒，气微香，味微苦。  滋养肝肾、宁心安神。用于更年期综合症属阴虚肝旺症，症见烘热汗出，头晕耳鸣，失眠多梦，五心烦热，腰背酸痛，大便干燥，心烦易怒，舌红少苔，脉弦细或弦细 开水冲服。一次1袋(12g)，一日3次。  如与其他药物同时使用可能会发生药物相互作用，详情请咨询医师或药师。  12g*10袋/盒  用于更年期综合症属阴虚肝旺症  铝塑复合膜包装，每袋装12克，每盒装10袋。  非处方药物（甲类）,中药保护品种二级  12g*10袋/盒  用于更年期综合症属阴虚肝旺更年期综合症气微香，味微苦。\"\n",
    "\n",
    "sent_chars = list(sentence)\n",
    "sent2id = [vocab2idx[word] if word in vocab2idx else vocab2idx['<UNK>'] for word in sent_chars]\n",
    "\n",
    "sent2id_new = np.array([[0] * (maxlen-len(sent2id)) + sent2id[:maxlen]])\n",
    "y_pred = model.predict(sent2id_new)\n",
    "y_label = np.argmax(y_pred, axis=2)\n",
    "y_label = y_label.reshape(1, -1)[0]\n",
    "y_ner = [idx2label[i] for i in y_label][-len(sent_chars):]\n",
    "\n",
    "print(idx2label)\n",
    "print(sent_chars)\n",
    "print(sent2id)\n",
    "print(y_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(sent_chars,y_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
