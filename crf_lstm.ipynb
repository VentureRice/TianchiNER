{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF+LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras 2.2.4\n",
    "\n",
    "tensorflow 1.13\n",
    "\n",
    "pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab_path = \"CRF/data/char_vocabs.txt\" # 字典文件\n",
    "#train_data_path = 'data/train_data/train_data_000' # 训练数据\n",
    "#train_data_path = './data/train_data' # 训练数据\n",
    "#test_data_path = 'data/train_data/train_data_000' # 测试数据\n",
    "\n",
    "special_words = ['<PAD>', '<UNK>'] # 特殊词表示\n",
    "\n",
    "# \"BIO\"标记的标签\n",
    "#label2idx = {\"O\": 0,\n",
    "#             \"B-PER\": 1, \"I-PER\": 2,\n",
    "#             \"B-LOC\": 3, \"I-LOC\": 4,\n",
    "#             \"B-ORG\": 5, \"I-ORG\": 6\n",
    "#            }\n",
    "label2idx = {'O': 0,\n",
    "             'B-DISEASE': 1, 'B-DISEASE_GROUP': 2,\n",
    "             'B-DRUG_DOSAGE': 3, 'B-DRUG_EFFICACY': 4,\n",
    "             'B-DRUG_INGREDIENT': 5, 'B-DRUG_TASTE': 6,\n",
    "             'B-FOOD_GROUP':7, 'B-PERSON_GROUP':8,\n",
    "             'B-SYMPTOM':9, 'B-SYNDROME':10,\n",
    "             'I-DISEASE': 11, 'I-DISEASE_GROUP': 12,\n",
    "             'I-DRUG_DOSAGE': 13, 'I-DRUG_EFFICACY': 14,\n",
    "             'I-DRUG_INGREDIENT': 15, 'I-DRUG_TASTE': 16,\n",
    "             'I-FOOD_GROUP':17, 'I-PERSON_GROUP':18,\n",
    "             'I-SYMPTOM':19, 'I-SYNDROME':20\n",
    "            }\n",
    "\n",
    "# 索引和BIO标签对应\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "\n",
    "# 读取字符词典文件\n",
    "with open(char_vocab_path, \"r\", encoding=\"utf8\") as fo:\n",
    "    char_vocabs = [line.strip() for line in fo]\n",
    "char_vocabs = special_words + char_vocabs\n",
    "\n",
    "# 字符和索引编号对应\n",
    "idx2vocab = {idx: char for idx, char in enumerate(char_vocabs)}\n",
    "vocab2idx = {char: idx for idx, char in idx2vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=' <>滋阴清热，健脾养血。用于放环后引起的出血，月经提前量多或月经紊乱，腰骶酸痛，下腹坠痛，心烦易怒，手足心热 陕西步长高新制药有限公司  口服，一次5片，一日2次。  请遵医嘱。  尚不明确。  0.46g*3*15片 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'___滋阴清热_健脾养血。用于放环后引起的出血_月经提前量多或月经紊乱_腰骶酸痛_下腹坠痛_心烦易怒_手足心热_陕西步长高新制药有限公司__口服_一次5片_一日2次。__请遵医嘱。__尚不明确。__0.46g_3_15片_'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(' |\\*|<|>|、|，','_',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练语料\n",
    "def read_corpus(corpus_path, vocab2idx, label2idx):\n",
    "    with open(corpus_path, encoding='utf-8') as fr:\n",
    "        lines = fr.readlines()\n",
    "\n",
    "    sent_, tag_ = [], []\n",
    "    for letter in lines:\n",
    "        [char,label,_] = re.split('\\t|\\n',letter)\n",
    "        char = re.sub(' |\\*|<|>|、|，','_',char)\n",
    "        sent_.append(char)\n",
    "        tag_.append(label)\n",
    "\n",
    "    sent_ids = [vocab2idx[char] if char in vocab2idx else vocab2idx['<UNK>'] for char in sent_]\n",
    "    tag_ids = [label2idx[label] if label in label2idx else 0 for label in tag_]\n",
    "    return sent_ids, tag_ids\n",
    "\n",
    "# 加载训练集\n",
    "#train_datas, train_labels = read_corpus(train_data_path, vocab2idx, label2idx)\n",
    "# 加载测试集\n",
    "#test_datas, test_labels = read_corpus(test_data_path, vocab2idx, label2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = []\n",
    "train_labels = []\n",
    "files = os.listdir('data/train_data')\n",
    "for file in files:\n",
    "    train_data_path_i = 'data/train_data/'+file\n",
    "    train_datas_i, train_labels_i = read_corpus(train_data_path_i, vocab2idx, label2idx)\n",
    "    train_datas.append(train_datas_i)\n",
    "    train_labels.append(train_labels_i)\n",
    "    #if i%10==0:\n",
    "    #    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datas = []\n",
    "valid_labels = []\n",
    "files = os.listdir('data/valid_data')\n",
    "for file in files:\n",
    "    valid_data_path_i = 'data/valid_data/'+file\n",
    "    valid_datas_i, valid_labels_i = read_corpus(valid_data_path_i, vocab2idx, label2idx)\n",
    "    valid_datas.append(valid_datas_i)\n",
    "    valid_labels.append(valid_labels_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 61, 77, 1, 58, 17, 58, 3093, 3817, 2654, 6214, 1959, 2177, 286, 58, 5965, 519, 1408, 2644, 2102, 2732, 1842, 889, 2545, 3093, 3817]\n",
      "['_', 'b', 'r', '<UNK>', '_', '3', '_', '治', '疗', '期', '间', '忌', '房', '事', '_', '配', '偶', '如', '有', '感', '染', '应', '同', '时', '治', '疗']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train_datas[50])\n",
    "print([idx2vocab[idx] for idx in train_datas[50]])\n",
    "print(train_labels[50])\n",
    "print([idx2label[idx] for idx in train_labels[50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "6874 21\n",
      "padding sequences\n",
      "x_train shape: (6899, 50)\n",
      "x_test shape: (3974, 50)\n",
      "trainlabels shape: (6899, 50, 21)\n",
      "testlabels shape: (3974, 50, 21)\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 48)            329952    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 16)            3648      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 21)            357       \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 50, 21)            945       \n",
      "=================================================================\n",
      "Total params: 334,902\n",
      "Trainable params: 334,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 6209 samples, validate on 690 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "6209/6209 [==============================] - 25s 4ms/step - loss: 3.5181 - crf_viterbi_accuracy: 0.7842 - val_loss: 3.0333 - val_crf_viterbi_accuracy: 0.8493\n",
      "Epoch 2/10\n",
      "6209/6209 [==============================] - 23s 4ms/step - loss: 2.9779 - crf_viterbi_accuracy: 0.8386 - val_loss: 2.9300 - val_crf_viterbi_accuracy: 0.8486\n",
      "Epoch 3/10\n",
      "6209/6209 [==============================] - 22s 4ms/step - loss: 2.8577 - crf_viterbi_accuracy: 0.8473 - val_loss: 2.8336 - val_crf_viterbi_accuracy: 0.8622\n",
      "Epoch 4/10\n",
      "5696/6209 [==========================>...] - ETA: 1s - loss: 2.7704 - crf_viterbi_accuracy: 0.8684"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Masking, Embedding, Bidirectional, LSTM, Dense, Input, TimeDistributed, Activation\n",
    "from keras.preprocessing import sequence\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 48\n",
    "HIDDEN_SIZE = 8\n",
    "MAX_LEN = 50\n",
    "VOCAB_SIZE = len(vocab2idx)\n",
    "CLASS_NUMS = len(label2idx)\n",
    "print(VOCAB_SIZE, CLASS_NUMS)\n",
    "\n",
    "print('padding sequences')\n",
    "train_datas = sequence.pad_sequences(train_datas, maxlen=MAX_LEN)\n",
    "train_labels = sequence.pad_sequences(train_labels, maxlen=MAX_LEN)\n",
    "valid_datas = sequence.pad_sequences(valid_datas, maxlen=MAX_LEN)\n",
    "valid_labels = sequence.pad_sequences(valid_labels, maxlen=MAX_LEN)\n",
    "print('x_train shape:', train_datas.shape)\n",
    "print('x_test shape:', valid_datas.shape)\n",
    "\n",
    "train_labels = keras.utils.to_categorical(train_labels, CLASS_NUMS)\n",
    "valid_labels = keras.utils.to_categorical(valid_labels, CLASS_NUMS)\n",
    "print('trainlabels shape:', train_labels.shape)\n",
    "print('testlabels shape:', valid_labels.shape)\n",
    "\n",
    "## BiLSTM+CRF模型构建\n",
    "inputs = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "x = Masking(mask_value=0)(inputs)\n",
    "x = Embedding(VOCAB_SIZE, EMBED_DIM, mask_zero=True)(x)\n",
    "x = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True))(x)\n",
    "x = TimeDistributed(Dense(CLASS_NUMS))(x)#TimeDistributed层的作用就是把Dense层应用到这10个具体的向量上，对每一个向量进行了一个Dense操作\n",
    "outputs = CRF(CLASS_NUMS)(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=crf_loss, optimizer='adam', metrics=[crf_viterbi_accuracy])\n",
    "model.fit(train_datas, train_labels, epochs=EPOCHS, verbose=1, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(valid_datas, valid_labels, batch_size=BATCH_SIZE)\n",
    "print(model.metrics_names)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"model/ch_ner_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_nertag(input_data, result_tags):\n",
    "    result_words = []\n",
    "    start, end =0, 1 # 实体开始结束位置标识\n",
    "    tag_label = \"O\" # 实体类型标识\n",
    "    number = 0\n",
    "    for i, tag in enumerate(result_tags):\n",
    "        if tag.startswith(\"B\"):\n",
    "            number += 1\n",
    "            if tag_label != \"O\": # 当前实体tag之前有其他实体     \n",
    "                result_words.append(('T'+str(number), tag_label, start, end,input_data[start: end]))\n",
    "                #result_words.append(('T'+str(number), tag_label+' '+str(start)+' '+str(end),input_data[start: end])) # 获取实体\n",
    "            tag_label = tag.split(\"-\")[1] # 获取当前实体类型\n",
    "            start, end = i, i+1 # 开始和结束位置变更\n",
    "        elif tag.startswith(\"I\"):\n",
    "            temp_label = tag.split(\"-\")[1]\n",
    "            if temp_label == tag_label: # 当前实体tag是之前实体的一部分\n",
    "                end += 1 # 结束位置end扩展\n",
    "        elif tag == \"O\":\n",
    "            if tag_label != \"O\": # 当前位置非实体 但是之前有实体\n",
    "                #result_words.append(('T'+str(number), tag_label+' '+str(start)+' '+str(end),input_data[start: end])) # 获取实体\n",
    "                result_words.append(('T'+str(number), tag_label, start, end,input_data[start: end]))\n",
    "                tag_label = \"O\"  # 实体类型置\"O\"\n",
    "            start, end = i, i+1 # 开始和结束位置变更\n",
    "    if tag_label != \"O\": # 最后结尾还有实体\n",
    "        number += 1\n",
    "        result_words.append(('T'+str(number), tag_label, start, end,input_data[start: end]))\n",
    "        #result_words.append(('T'+str(number),tag_label+' '+str(start)+' '+str(end),input_data[start: end])) # 获取结尾的实体\n",
    "    return result_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "result = {}\n",
    "test_data_path = 'data/chusai_xuanshou/'\n",
    "for i in range(500):\n",
    "    test_file = test_data_path+str(i+1000)+'.txt'\n",
    "    with open(test_file, \"r\", encoding=\"utf8\") as test:\n",
    "        sentence = test.read()\n",
    "    sentences = sentence.split('。')\n",
    "    y_ner = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent = sent.replace(' ','_')\n",
    "        sent_chars = list(sent+'。')\n",
    "        sent2id = [vocab2idx[word] if word in vocab2idx else vocab2idx['<UNK>'] for word in sent_chars]\n",
    "\n",
    "        sent2id_new = np.array([[0] * (maxlen-len(sent2id)) + sent2id[:maxlen]])\n",
    "        y_pred = model.predict(sent2id_new)\n",
    "        y_label = np.argmax(y_pred, axis=2)\n",
    "        y_label = y_label.reshape(1, -1)[0]\n",
    "        y_ner_ = [idx2label[i] for i in y_label][-len(sent_chars):]\n",
    "        y_ner.extend(y_ner_)\n",
    "    result_words = get_valid_nertag(sentence, y_ner)\n",
    "    ans = []\n",
    "    for res in result_words:\n",
    "        number = res[0]\n",
    "        #tag_start_end = res[1]\n",
    "        tag = res[1]\n",
    "        start = res[2]\n",
    "        end = res[3]\n",
    "        word = res[4].replace(' ','_')\n",
    "        ans.append('{}\\t{} {} {}\\t{}'.format('T'+str(len(ans)+1), tag,start, end-1, word))\n",
    "        #ans.append(, tag, start,end, \"\".join(word))\n",
    "        print('{}\\t{} {} {}\\t{}'.format('T'+str(len(ans)), tag,start, end-1, word))\n",
    "    result[i+1000] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#“实体类别”、“起始位置”、“结束位置”以空格分隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000,1500):\n",
    "    with open('data/submit/%d.ann'%i,'w', encoding='utf-8') as wr:\n",
    "        wr.write('\\n'.join(result[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_data/1.ann', encoding='utf-8') as wr:\n",
    "    test = wr.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(result[1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1000,1500):\n",
    "#    pd.DataFrame(result[i]).to_csv('data/submit/%d.ann'%i,\n",
    "#                                      sep='\\t',\n",
    "#                                      header = None,\n",
    "#                                      index = 0,\n",
    "#                                      encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
