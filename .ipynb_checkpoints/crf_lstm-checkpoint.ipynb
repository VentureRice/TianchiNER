{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF+LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras 2.2.4\n",
    "\n",
    "tensorflow 1.13\n",
    "\n",
    "pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab_path = \"CRF/data/char_vocabs.txt\" # 字典文件\n",
    "#train_data_path = 'data/train_data/train_data_000' # 训练数据\n",
    "#train_data_path = './data/train_data' # 训练数据\n",
    "#test_data_path = 'data/train_data/train_data_000' # 测试数据\n",
    "\n",
    "special_words = ['<PAD>', '<UNK>'] # 特殊词表示\n",
    "\n",
    "# \"BIO\"标记的标签\n",
    "#label2idx = {\"O\": 0,\n",
    "#             \"B-PER\": 1, \"I-PER\": 2,\n",
    "#             \"B-LOC\": 3, \"I-LOC\": 4,\n",
    "#             \"B-ORG\": 5, \"I-ORG\": 6\n",
    "#            }\n",
    "label2idx = {'O': 0,\n",
    "             'DISEASE': 1, 'DISEASE_GROUP': 2,\n",
    "             'DRUG_DOSAGE': 3, 'DRUG_EFFICACY': 4,\n",
    "             'DRUG_INGREDIENT': 5, 'DRUG_TASTE': 6,\n",
    "             'FOOD_GROUP':7, 'PERSON_GROUP':8,\n",
    "             'SYMPTOM':9, 'SYNDROME':10\n",
    "            }\n",
    "\n",
    "# 索引和BIO标签对应\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "\n",
    "# 读取字符词典文件\n",
    "with open(char_vocab_path, \"r\", encoding=\"utf8\") as fo:\n",
    "    char_vocabs = [line.strip() for line in fo]\n",
    "char_vocabs = special_words + char_vocabs\n",
    "\n",
    "# 字符和索引编号对应\n",
    "idx2vocab = {idx: char for idx, char in enumerate(char_vocabs)}\n",
    "vocab2idx = {char: idx for idx, char in idx2vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练语料\n",
    "def read_corpus(corpus_path, vocab2idx, label2idx):\n",
    "    with open(corpus_path, encoding='utf-8') as fr:\n",
    "        lines = fr.readlines()\n",
    "\n",
    "    sent_, tag_ = [], []\n",
    "    for letter in lines:\n",
    "        [char,label,_] = re.split('\\t|\\n',letter)\n",
    "        sent_.append(char)\n",
    "        tag_.append(label)\n",
    "\n",
    "    sent_ids = [vocab2idx[char] if char in vocab2idx else vocab2idx['<UNK>'] for char in sent_]\n",
    "    tag_ids = [label2idx[label] if label in label2idx else 0 for label in tag_]\n",
    "    return sent_ids, tag_ids\n",
    "\n",
    "# 加载训练集\n",
    "#train_datas, train_labels = read_corpus(train_data_path, vocab2idx, label2idx)\n",
    "# 加载测试集\n",
    "#test_datas, test_labels = read_corpus(test_data_path, vocab2idx, label2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = []\n",
    "train_labels = []\n",
    "files = os.listdir('data/train_data')\n",
    "for file in files:\n",
    "    train_data_path_i = 'data/train_data/'+file\n",
    "    train_datas_i, train_labels_i = read_corpus(train_data_path_i, vocab2idx, label2idx)\n",
    "    train_datas.append(train_datas_i)\n",
    "    train_labels.append(train_labels_i)\n",
    "    #if i%10==0:\n",
    "    #    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datas = []\n",
    "valid_labels = []\n",
    "files = os.listdir('data/valid_data')\n",
    "for file in files:\n",
    "    valid_data_path_i = 'data/valid_data/'+file\n",
    "    valid_datas_i, valid_labels_i = read_corpus(valid_data_path_i, vocab2idx, label2idx)\n",
    "    valid_datas.append(valid_datas_i)\n",
    "    valid_labels.append(valid_labels_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4955, 3769, 44, 50, 31, 4062, 3541, 6802, 4955, 998, 759, 5297, 3769, 44, 48, 44, 6089, 4303, 6817, 15, 16, 4352, 9, 16, 2698, 1, 3929, 182, 0, 0, 886, 2706, 6010, 1578, 4955, 229, 4672, 355, 2644, 6268, 580, 876, 0, 0, 14, 13, 17, 66, 9, 16, 18, 4352, 1, 3929, 0, 1537, 1412, 1959, 3769, 182, 0, 0, 3769, 288, 3009, 5133, 5253, 3869, 2178, 4793, 3903, 2643, 4471, 219, 5483, 181, 3853, 4471, 181, 1953, 1, 6802, 2118, 2002, 3921, 4747, 3418, 5347, 4760, 2644, 1953, 1239, 181, 5256, 4471, 4760, 3853, 5428, 490, 4622, 0, 0, 3923, 3009, 3153, 5253, 6802, 2490, 4475, 2953, 3853, 182, 3769, 288, 3009, 5133, 5253, 3869, 2178, 4793, 3903, 2643, 4471, 219, 5483, 3009, 5133, 5253, 3869, 2178, 4793, 3903, 2643, 4471, 219, 5483, 181, 3853, 4471, 181, 1953, 1, 181, 3844, 5347, 5256, 4471, 891, 6133, 181, 4471, 6009, 1637, 2644, 5253, 1239, 181, 4471, 5256, 1635, 4760, 3839, 3853, 181, 4760, 2644, 1953, 1239, 6817, 2118, 2002, 3921, 4747, 3418, 4760, 2644, 1953, 1239, 3009, 1951, 6508, 0, 2659, 998, 249, 4062, 4715, 1188, 6802, 596, 1596, 3556, 249, 2824, 5320, 4838, 2161, 6740, 5320, 4838, 6439, 4352, 6817, 3009, 1951, 6508, 6802, 954, 4894, 181, 1951, 993, 954, 4894, 181, 1951, 993, 182, 0, 0, 857, 2646, 182, 211, 2937, 18, 12, 20, 4352, 6802, 211, 2535, 16, 12, 17, 2937, 182, 0, 0, 15, 0, 4955, 2477, 1549, 1579, 6541, 5270, 2558, 6816, 2659, 998, 589, 2644, 2558, 2579, 3903, 2223, 3418, 6164, 3853, 406, 3769, 6802, 889, 2545, 4719, 451, 5801, 3765, 2974, 4385, 4486, 1951, 1950, 3647, 6802, 765, 5361, 398, 596, 4687, 1239, 6802, 1825, 2644, 1338, 1903, 566, 3827, 715, 3903, 406, 3769, 6817, 16, 0, 398, 1361, 5442, 6541, 2579, 4110, 6816, 1622, 4793, 3843, 2002, 1367, 4671, 2671, 4988, 6802, 6010, 6735, 4838, 5029, 5006, 3680, 4988, 589, 2644, 2558, 2579, 3903, 2217, 680, 966, 2668, 3404, 406, 3769, 6817, 17, 0, 2992, 3682, 5442, 6541, 5270, 2558, 6816, 5457, 4955, 1566, 577, 868, 6363, 6802, 705, 406, 3769, 6802, 245, 1837, 2646, 3769, 688, 6009, 1232, 249, 1566, 577, 182, 0, 0, 1408, 220, 588, 332, 4955, 3556, 889, 2545, 424, 3769, 868, 4719, 369, 847, 3765, 4955, 3556, 3944, 291, 406, 3769, 6802, 5458, 2065, 5474, 983, 5454, 776, 1788, 2161, 4955, 1788, 182, 0, 0, 1208, 1594, 776, 463, 3939, 1908, 6798, 270, 4346, 6799, 11, 242, 4955, 463, 2231, 998, 4166, 287, 4439, 0]\n",
      "['<PAD>', '药', '用', 'P', 'V', 'C', '硬', '片', '，', '药', '品', '包', '装', '用', 'P', 'T', 'P', '铝', '箔', '；', '1', '2', '粒', '*', '2', '板', '<UNK>', '盒', '。', '<PAD>', '<PAD>', '吉', '林', '金', '宝', '药', '业', '股', '份', '有', '限', '公', '司', '<PAD>', '<PAD>', '0', '.', '3', 'g', '*', '2', '4', '粒', '<UNK>', '盒', '<PAD>', '孕', '妇', '忌', '用', '。', '<PAD>', '<PAD>', '用', '于', '气', '虚', '血', '瘀', '所', '致', '的', '月', '经', '不', '调', '、', '痛', '经', '、', '徵', '<UNK>', '，', '慢', '性', '盆', '腔', '炎', '见', '腹', '有', '徵', '块', '、', '行', '经', '腹', '痛', '证', '候', '者', '<PAD>', '<PAD>', '益', '气', '活', '血', '，', '散', '结', '止', '痛', '。', '用', '于', '气', '虚', '血', '瘀', '所', '致', '的', '月', '经', '不', '调', '气', '虚', '血', '瘀', '所', '致', '的', '月', '经', '不', '调', '、', '痛', '经', '、', '徵', '<UNK>', '、', '症', '见', '行', '经', '后', '错', '、', '经', '量', '少', '有', '血', '块', '、', '经', '行', '小', '腹', '疼', '痛', '、', '腹', '有', '徵', '块', '；', '慢', '性', '盆', '腔', '炎', '腹', '有', '徵', '块', '气', '微', '香', '<PAD>', '本', '品', '为', '硬', '胶', '囊', '，', '内', '容', '物', '为', '棕', '褐', '色', '或', '黑', '褐', '色', '颗', '粒', '；', '气', '微', '香', '，', '味', '苦', '、', '微', '咸', '味', '苦', '、', '微', '咸', '。', '<PAD>', '<PAD>', '口', '服', '。', '一', '次', '4', '-', '6', '粒', '，', '一', '日', '2', '-', '3', '次', '。', '<PAD>', '<PAD>', '1', '<PAD>', '药', '效', '学', '实', '验', '表', '明', '：', '本', '品', '具', '有', '明', '显', '的', '抗', '炎', '镇', '痛', '作', '用', '，', '同', '时', '能', '促', '进', '生', '殖', '系', '统', '微', '循', '环', '，', '化', '解', '体', '内', '肿', '块', '，', '并', '有', '增', '强', '免', '疫', '力', '的', '作', '用', '；', '2', '<PAD>', '体', '外', '试', '验', '显', '示', '：', '对', '致', '病', '性', '大', '肠', '杆', '菌', '，', '金', '黄', '色', '葡', '萄', '球', '菌', '具', '有', '明', '显', '的', '抑', '制', '和', '杀', '灭', '作', '用', '；', '3', '<PAD>', '毒', '理', '试', '验', '表', '明', '：', '该', '药', '安', '全', '可', '靠', '，', '副', '作', '用', '，', '临', '床', '服', '用', '剂', '量', '均', '为', '安', '全', '。', '<PAD>', '<PAD>', '如', '与', '其', '他', '药', '物', '同', '时', '使', '用', '可', '能', '会', '发', '生', '药', '物', '相', '互', '作', '用', '，', '详', '情', '请', '咨', '询', '医', '师', '或', '药', '师', '。', '<PAD>', '<PAD>', '国', '家', '医', '保', '目', '录', '（', '乙', '类', '）', ',', '中', '药', '保', '护', '品', '种', '二', '级', '<PAD>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 10, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 0, 9, 9, 0, 1, 1, 1, 1, 1, 0, 9, 9, 9, 9, 0, 9, 9, 9, 9, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 10, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 0, 9, 9, 0, 0, 0, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 9, 9, 9, 9, 6, 6, 6, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PERSON_GROUP', 'PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'SYNDROME', 'SYNDROME', 'SYNDROME', 'SYNDROME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'SYMPTOM', 'SYMPTOM', 'O', 'SYMPTOM', 'SYMPTOM', 'O', 'DISEASE', 'DISEASE', 'DISEASE', 'DISEASE', 'DISEASE', 'O', 'SYMPTOM', 'SYMPTOM', 'SYMPTOM', 'SYMPTOM', 'O', 'SYMPTOM', 'SYMPTOM', 'SYMPTOM', 'SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'SYNDROME', 'SYNDROME', 'SYNDROME', 'SYNDROME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'SYMPTOM', 'SYMPTOM', 'O', 'SYMPTOM', 'SYMPTOM', 'O', 'O', 'O', 'SYMPTOM', 'SYMPTOM', 'SYMPTOM', 'SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'SYMPTOM', 'SYMPTOM', 'SYMPTOM', 'SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'DISEASE', 'DISEASE', 'DISEASE', 'DISEASE', 'DISEASE', 'SYMPTOM', 'SYMPTOM', 'SYMPTOM', 'SYMPTOM', 'DRUG_TASTE', 'DRUG_TASTE', 'DRUG_TASTE', 'O', 'O', 'O', 'O', 'O', 'DRUG_DOSAGE', 'DRUG_DOSAGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DRUG_TASTE', 'DRUG_TASTE', 'DRUG_TASTE', 'DRUG_TASTE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'DRUG_EFFICACY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train_datas[50])\n",
    "print([idx2vocab[idx] for idx in train_datas[50]])\n",
    "print(train_labels[50])\n",
    "print([idx2label[idx] for idx in train_labels[50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6874 11\n",
      "padding sequences\n",
      "x_train shape: (601, 500)\n",
      "x_test shape: (399, 500)\n",
      "trainlabels shape: (601, 500, 11)\n",
      "testlabels shape: (399, 500, 11)\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 32)           219968    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 500, 32)           6272      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 500, 11)           363       \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 500, 11)           275       \n",
      "=================================================================\n",
      "Total params: 226,878\n",
      "Trainable params: 226,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 540 samples, validate on 61 samples\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "540/540 [==============================] - 16s 30ms/step - loss: 6.8674 - crf_viterbi_accuracy: 0.3958 - val_loss: 4.9212 - val_crf_viterbi_accuracy: 0.4061\n",
      "Epoch 2/15\n",
      "192/540 [=========>....................] - ETA: 8s - loss: 6.4862 - crf_viterbi_accuracy: 0.4118"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Masking, Embedding, Bidirectional, LSTM, Dense, Input, TimeDistributed, Activation\n",
    "from keras.preprocessing import sequence\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 100\n",
    "EMBED_DIM = 32\n",
    "HIDDEN_SIZE = 16\n",
    "MAX_LEN = 500\n",
    "VOCAB_SIZE = len(vocab2idx)\n",
    "CLASS_NUMS = len(label2idx)\n",
    "print(VOCAB_SIZE, CLASS_NUMS)\n",
    "\n",
    "print('padding sequences')\n",
    "train_datas = sequence.pad_sequences(train_datas, maxlen=MAX_LEN)\n",
    "train_labels = sequence.pad_sequences(train_labels, maxlen=MAX_LEN)\n",
    "valid_datas = sequence.pad_sequences(valid_datas, maxlen=MAX_LEN)\n",
    "valid_labels = sequence.pad_sequences(valid_labels, maxlen=MAX_LEN)\n",
    "print('x_train shape:', train_datas.shape)\n",
    "print('x_test shape:', valid_datas.shape)\n",
    "\n",
    "train_labels = keras.utils.to_categorical(train_labels, CLASS_NUMS)\n",
    "valid_labels = keras.utils.to_categorical(valid_labels, CLASS_NUMS)\n",
    "print('trainlabels shape:', train_labels.shape)\n",
    "print('testlabels shape:', valid_labels.shape)\n",
    "\n",
    "## BiLSTM+CRF模型构建\n",
    "inputs = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "x = Masking(mask_value=0)(inputs)\n",
    "x = Embedding(VOCAB_SIZE, EMBED_DIM, mask_zero=True)(x)\n",
    "x = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True))(x)\n",
    "x = TimeDistributed(Dense(CLASS_NUMS))(x)\n",
    "outputs = CRF(CLASS_NUMS)(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=crf_loss, optimizer='adam', metrics=[crf_viterbi_accuracy])\n",
    "model.fit(train_datas, train_labels, epochs=EPOCHS, verbose=1, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(valid_datas, valid_labels, batch_size=BATCH_SIZE)\n",
    "print(model.metrics_names)\n",
    "print(score)\n",
    "\n",
    "# save model\n",
    "model.save(\"./model/ch_ner_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 500\n",
    "sentence = \" 北京同仁堂科技发展股份有限公司制药厂  1.忌食辛辣，少进油腻。 2.感冒发热病人不宜服用。 3.有高血压、心脏病、肝病、糖尿病、肾病等慢性病严重者应在医师指导下服用。 4.伴有月经紊乱者，应在医师指导下服用。 5.眩晕症状较重者，应及时去医院就诊。 6.服药2周症状无缓解，应去医院就诊。 7.对本品过敏者禁用，过敏体质者慎用。 8.本品性状发生改变时禁止使用。 9.请将本品放在儿童不能接触的地方。 10.如正在使用其他药品，使用本品前请咨询医师或药师。  本品为浅黄色至棕黄色颗粒，气微香，味微苦。  滋养肝肾、宁心安神。用于更年期综合症属阴虚肝旺症，症见烘热汗出，头晕耳鸣，失眠多梦，五心烦热，腰背酸痛，大便干燥，心烦易怒，舌红少苔，脉弦细或弦细 开水冲服。一次1袋(12g)，一日3次。  如与其他药物同时使用可能会发生药物相互作用，详情请咨询医师或药师。  12g*10袋/盒  用于更年期综合症属阴虚肝旺症  铝塑复合膜包装，每袋装12克，每盒装10袋。  非处方药物（甲类）,中药保护品种二级  12g*10袋/盒  用于更年期综合症属阴虚肝旺更年期综合症气微香，味微苦。\"\n",
    "\n",
    "sent_chars = list(sentence)\n",
    "sent2id = [vocab2idx[word] if word in vocab2idx else vocab2idx['<UNK>'] for word in sent_chars]\n",
    "\n",
    "sent2id_new = np.array([[0] * (maxlen-len(sent2id)) + sent2id[:maxlen]])\n",
    "y_pred = model.predict(sent2id_new)\n",
    "y_label = np.argmax(y_pred, axis=2)\n",
    "y_label = y_label.reshape(1, -1)[0]\n",
    "y_ner = [idx2label[i] for i in y_label][-len(sent_chars):]\n",
    "\n",
    "print(idx2label)\n",
    "print(sent_chars)\n",
    "print(sent2id)\n",
    "print(y_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(sent_chars,y_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
